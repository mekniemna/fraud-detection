"""
model_training.py - Script de comparaison de mod√®les (version corrig√©e)
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, 
    f1_score, roc_auc_score, confusion_matrix,
    classification_report
)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import (
    RandomForestClassifier,
    GradientBoostingClassifier
)
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import seaborn as sns
import time
import warnings
import joblib

warnings.filterwarnings('ignore')

class FraudDetectionModels:
    def __init__(self, data_path):
        self.data_path = data_path
        self.df = None
        self.X_train, self.X_test, self.y_train, self.y_test = None, None, None, None
        self.scaler = StandardScaler()
        self.models = {}
        self.results = pd.DataFrame(columns=[
            'Model', 'Accuracy', 'Precision', 'Recall', 
            'F1', 'ROC AUC', 'Training Time'
        ])
        
    def load_and_prepare_data(self):
        """Charge et pr√©pare les donn√©es pour l'entra√Ænement"""
        print("\n=== CHARGEMENT DES DONN√âES ===")
        self.df = pd.read_csv(self.data_path)
        
        # S√©paration features/target
        X = self.df.drop(['job_id', 'title', 'location', 'department', 'fraudulent'], axis=1, errors='ignore')
        y = self.df['fraudulent']
        
        # Split train/test
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        
        # Normalisation
        self.X_train = self.scaler.fit_transform(self.X_train)
        self.X_test = self.scaler.transform(self.X_test)
        
        print(f"‚úÖ Donn√©es pr√©par√©es: {X.shape[1]} features")
        print(f"üìä R√©partition des classes: {np.mean(y):.2%} de fraudes")
        
    def initialize_models(self):
        """Initialise les mod√®les √† comparer"""
        self.models = {
            "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced'),
            "Random Forest": RandomForestClassifier(class_weight='balanced', random_state=42),
            "Gradient Boosting": GradientBoostingClassifier(random_state=42),
            "XGBoost": XGBClassifier(scale_pos_weight=np.sum(self.y_train==0)/np.sum(self.y_train==1), 
                                   random_state=42),
            "LightGBM": LGBMClassifier(class_weight='balanced', random_state=42),
            "SVM": SVC(class_weight='balanced', probability=True, random_state=42)
        }
        
    def train_and_evaluate(self):
        """Entra√Æne et √©value tous les mod√®les"""
        print("\n=== ENTRA√éNEMENT DES MOD√àLES ===")
        for name, model in self.models.items():
            start_time = time.time()
            
            # Entra√Ænement
            model.fit(self.X_train, self.y_train)
            
            # Pr√©dictions
            y_pred = model.predict(self.X_test)
            y_proba = model.predict_proba(self.X_test)[:, 1]
            
            # Calcul des m√©triques
            train_time = time.time() - start_time
            accuracy = accuracy_score(self.y_test, y_pred)
            precision = precision_score(self.y_test, y_pred)
            recall = recall_score(self.y_test, y_pred)
            f1 = f1_score(self.y_test, y_pred)
            roc_auc = roc_auc_score(self.y_test, y_proba)
            
            # Stockage des r√©sultats (version corrig√©e)
            new_row = pd.DataFrame({
                'Model': [name],
                'Accuracy': [accuracy],
                'Precision': [precision],
                'Recall': [recall],
                'F1': [f1],
                'ROC AUC': [roc_auc],
                'Training Time': [train_time]
            })
            self.results = pd.concat([self.results, new_row], ignore_index=True)
            
            print(f"\nüìä {name} - Performance:")
            print(f"Accuracy: {accuracy:.4f} | Precision: {precision:.4f}")
            print(f"Recall: {recall:.4f} | F1: {f1:.4f}")
            print(f"ROC AUC: {roc_auc:.4f} | Time: {train_time:.2f}s")
            
            # Matrice de confusion
            self.plot_confusion_matrix(y_pred, name)
            
            # Rapport de classification
            print(f"\nüìù Rapport de classification pour {name}:")
            print(classification_report(self.y_test, y_pred))
    
    def plot_confusion_matrix(self, y_pred, model_name):
        """Affiche la matrice de confusion"""
        cm = confusion_matrix(self.y_test, y_pred)
        plt.figure(figsize=(6, 4))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=['L√©gitime', 'Fraude'],
                    yticklabels=['L√©gitime', 'Fraude'])
        plt.title(f'Matrice de confusion - {model_name}')
        plt.ylabel('V√©rit√© terrain')
        plt.xlabel('Pr√©dictions')
        plt.show()
    
    def compare_results(self):
        """Compare les performances des mod√®les"""
        print("\n=== COMPARAISON DES MOD√àLES ===")
        plt.figure(figsize=(12, 8))
        
        # Pr√©paration des donn√©es pour le graphique
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC']
        melted_results = self.results.melt(id_vars='Model', 
                                         value_vars=metrics,
                                         var_name='Metric', 
                                         value_name='Value')
        
        # Graphique de comparaison
        sns.barplot(x='Model', y='Value', hue='Metric', data=melted_results)
        plt.title('Comparaison des performances des mod√®les')
        plt.xticks(rotation=45)
        plt.legend(loc='upper left')
        plt.tight_layout()
        plt.show()
        
        # Affichage des r√©sultats
        print("\nüîç Tableau comparatif:")
        print(self.results.sort_values('F1', ascending=False))
        
        # Sauvegarde des r√©sultats
        self.results.to_csv('model_comparison_results.csv', index=False)
        print("\nüíæ R√©sultats sauvegard√©s dans model_comparison_results.csv")
    
    def save_best_model(self):
        """Sauvegarde le meilleur mod√®le"""
        best_model_name = self.results.sort_values('F1', ascending=False).iloc[0]['Model']
        best_model = self.models[best_model_name]
        
        joblib.dump(best_model, 'best_model.pkl')
        joblib.dump(self.scaler, 'scaler.pkl')
        
        print(f"\nüèÜ Meilleur mod√®le sauvegard√©: {best_model_name} (best_model.pkl)")
    
    def run(self):
        """Ex√©cute le pipeline complet"""
        self.load_and_prepare_data()
        self.initialize_models()
        self.train_and_evaluate()
        self.compare_results()
        self.save_best_model()

if __name__ == "__main__":
    print("""
    ====================================
    COMPARAISON DE MOD√àLES - D√âTECTION DE FRAUDE
    ====================================
    """)
    
    # Chemin vers vos donn√©es pr√©par√©es
    DATA_PATH = "../data/clean_jobs.csv"
    
    trainer = FraudDetectionModels(DATA_PATH)
    trainer.run()